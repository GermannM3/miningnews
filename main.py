import asyncio
import logging
import hashlib
import os
import random
import re
import ssl
from datetime import datetime
from html import escape
from typing import List, Dict, Optional, Set
import aiohttp
import httpx
import cloudscraper
import feedparser
import chardet
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from aiogram import Bot
from aiogram.enums import ParseMode
from deep_translator import GoogleTranslator
from langdetect import detect, LangDetectException

from config import (
    BOT_TOKEN,
    CHANNEL_ID,
    PREVIEW_CHANNEL_ID,
    CHECK_INTERVAL,
    MAX_NEWS_PER_SOURCE,
    DUPLICATES_FILE,
    STATIC_PROXY,
    PROXY_SOURCE_URL,
    PLAYWRIGHT_HEADLESS,
    PLAYWRIGHT_TIMEOUT,
)
from sources import NEWS_SOURCES
from filters import is_relevant

logging.basicConfig(
    level=logging.INFO,  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å DEBUG –Ω–∞ INFO –¥–ª—è –º–µ–Ω–µ–µ —à—É–º–Ω—ã—Ö –ª–æ–≥–æ–≤
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º DEBUG –ª–æ–≥–∏ –æ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ HTTP
for lib_name in ["httpx", "httpcore", "urllib3", "aiohttp", "asyncio"]:
    lib_logger = logging.getLogger(lib_name)
    lib_logger.setLevel(logging.WARNING)

if not BOT_TOKEN or not CHANNEL_ID:
    logger.error("BOT_TOKEN –∏ CHANNEL_ID –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
    raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

bot = Bot(token=BOT_TOKEN)

EMOJIS = {
    "start": ["üè≠", "‚öôÔ∏è", "üî•", "üìä", "üåç", "üí°"],
    "end": ["üîó", "üì∞", "‚ú®", "üöÄ", "‚≠ê"]
}

PROXY_POOL: List[str] = []
PROXY_INDEX = 0


def load_proxy_pool():
    global PROXY_POOL
    if not PROXY_SOURCE_URL or PROXY_POOL:
        return
    try:
        response = httpx.get(PROXY_SOURCE_URL, timeout=10.0)
        if response.status_code == 200:
            proxies = [line.strip() for line in response.text.splitlines() if line.strip()]
            if proxies:
                random.shuffle(proxies)
                PROXY_POOL = proxies
                logger.info(f"üåê –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–∫—Å–∏: {len(PROXY_POOL)} —à—Ç.")
            else:
                logger.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –ø—É—Å—Ç")
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–∫—Å–∏, —Å—Ç–∞—Ç—É—Å {response.status_code}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∫—Å–∏: {e}")


def get_next_proxy() -> Optional[str]:
    global PROXY_INDEX
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
    if STATIC_PROXY:
        return STATIC_PROXY
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Å–ø–∏—Å–∫–∞
    if not PROXY_SOURCE_URL:
        return None
    if not PROXY_POOL:
        load_proxy_pool()
    if not PROXY_POOL:
        return None
    proxy = PROXY_POOL[PROXY_INDEX % len(PROXY_POOL)]
    PROXY_INDEX += 1
    if not proxy.startswith("http"):
        proxy = f"http://{proxy}"
    return proxy


def cleanup_logs():
    log_path = "bot.log"
    try:
        if os.path.exists(log_path):
            os.remove(log_path)
            logger.debug("üßπ –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤: —Ñ–∞–π–ª bot.log —É–¥–∞–ª–µ–Ω")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –ª–æ–≥–æ–≤ {log_path}: {e}")


def sanitize_feed_content(content: str) -> str:
    if not content:
        return content
    cleaned = content.replace("\xa0", " ").replace("&nbsp;", " ")
    cleaned = re.sub(r"&(?![a-zA-Z]+;|#\d+;)", "&amp;", cleaned)
    return cleaned


def clean_title(title: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –¥–∞—Ç, –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π"""
    if not title:
        return title
    
    original_title = title  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ—á–∏—Å—Ç–∫–∞ —É–¥–∞–ª–∏—Ç –≤—Å—ë
    
    # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    title = re.sub(r'\d{1,2}\s+(—è–Ω–≤–∞—Ä[—å—è]|—Ñ–µ–≤—Ä–∞–ª[—å—è]|–º–∞—Ä—Ç[–∞]?|–∞–ø—Ä–µ–ª[—å—è]|–º–∞[–π—è]|–∏—é–Ω[—å—è]|–∏—é–ª[—å—è]|–∞–≤–≥—É—Å—Ç[–∞]?|—Å–µ–Ω—Ç—è–±—Ä[—å—è]|–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–¥–µ–∫–∞–±—Ä[—å—è])\s+\d{4}\s*', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\d{1,2}[./]\d{1,2}[./]\d{2,4}\s*', '', title)
    title = re.sub(r'\d{4}\s*–≥\.\s*', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\d{4}\s*–≥–æ–¥[–∞]?\s*', '', title, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –Ω–∞—á–∞–ª–µ
    title = re.sub(r'^(–ü—Ä–æ–¥—É–∫—Ü–∏—è|–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è|–£—Å—Ç–æ–π—á–∏–≤–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ|–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞|IR|–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ|–û –ø—Ä–∏–Ω—è—Ç–∏–∏)\s*[/|]\s*', '', title, flags=re.IGNORECASE)
    title = re.sub(r'^\d{1,2}\s+(–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–º–∞—Ä—Ç[–∞]?|–∞–ø—Ä–µ–ª[—å—è])\s+\d{4}\s*–≥\.\s*', '', title, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã
    title = re.sub(r'\s*\[PDF.*?\]\s*$', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\s*\(PDF.*?\)\s*$', '', title, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    title = re.sub(r'\s+', ' ', title).strip()
    
    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –¥–∞—Ç—ã, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏
    if re.match(r'^\d{1,2}\s+(—è–Ω–≤–∞—Ä[—å—è]|—Ñ–µ–≤—Ä–∞–ª[—å—è]|–º–∞—Ä—Ç[–∞]?|–∞–ø—Ä–µ–ª[—å—è]|–º–∞[–π—è]|–∏—é–Ω[—å—è]|–∏—é–ª[—å—è]|–∞–≤–≥—É—Å—Ç[–∞]?|—Å–µ–Ω—Ç—è–±—Ä[—å—è]|–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–¥–µ–∫–∞–±—Ä[—å—è])\s+\d{4}', title, flags=re.IGNORECASE):
        parts = title.split('.', 1)
        if len(parts) > 1 and len(parts[1].strip()) > 10:
            title = parts[1].strip()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    if not title or len(title.strip()) < 3:
        return original_title.strip()
    
    return title


def clean_description(description: str, title: str = '') -> str:
    """–û—á–∏—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    if not description:
        return description
    
    # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏, –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
    description = re.sub(r'<[^>]+>', '', description)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ –Ω–∞—á–∞–ª–µ –æ–ø–∏—Å–∞–Ω–∏—è
    if title:
        title_normalized = re.sub(r'[^\w\s]', '', title.lower()).strip()
        desc_normalized = re.sub(r'[^\w\s]', '', description.lower()).strip()
        
        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞, —É–±–∏—Ä–∞–µ–º –µ–≥–æ
        if desc_normalized.startswith(title_normalized):
            title_words = title_normalized.split()
            desc_words_orig = description.split()
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–≤
            match_count = 0
            for i, word in enumerate(title_words[:min(5, len(title_words))]):
                if i < len(desc_words_orig) and desc_words_orig[i].lower() == word:
                    match_count += 1
                else:
                    break
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–µ 3+ —Å–ª–æ–≤–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            if match_count >= 3 and len(desc_words_orig) > match_count:
                description = ' '.join(desc_words_orig[match_count:])
    
    # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã –≤ –Ω–∞—á–∞–ª–µ
    description = re.sub(r'^\d{1,2}\s+(—è–Ω–≤–∞—Ä[—å—è]|—Ñ–µ–≤—Ä–∞–ª[—å—è]|–º–∞—Ä—Ç[–∞]?|–∞–ø—Ä–µ–ª[—å—è]|–º–∞[–π—è]|–∏—é–Ω[—å—è]|–∏—é–ª[—å—è]|–∞–≤–≥—É—Å—Ç[–∞]?|—Å–µ–Ω—Ç—è–±—Ä[—å—è]|–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–¥–µ–∫–∞–±—Ä[—å—è])\s+\d{4}\s*–≥\.\s*', '', description, flags=re.IGNORECASE)
    description = re.sub(r'^\d{1,2}[./]\d{1,2}[./]\d{2,4}\s*', '', description)
    
    # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    description = re.sub(r'^(–ü—Ä–æ–¥—É–∫—Ü–∏—è|–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è|–£—Å—Ç–æ–π—á–∏–≤–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ|–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞|IR|–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ|–û –ø—Ä–∏–Ω—è—Ç–∏–∏)\s*[/|]\s*', '', description, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    description = re.sub(r'\s+', ' ', description).strip()
    
    return description


async def fetch_html_with_playwright(url: str, source: Dict) -> Optional[str]:
    try:
        proxy_config = None
        static_proxy = get_next_proxy()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è Playwright (—Ç–æ–ª—å–∫–æ –¥–ª—è SOCKS5 –∏–ª–∏ HTTP)
        if static_proxy:
            if static_proxy.startswith('socks5://') or static_proxy.startswith('http://'):
                proxy_config = {"server": static_proxy}
        
        async with async_playwright() as playwright:
            browser_args = ["--no-sandbox", "--disable-dev-shm-usage"]
            browser = await playwright.chromium.launch(
                headless=PLAYWRIGHT_HEADLESS,
                args=browser_args,
                proxy=proxy_config,
            )
            context = await browser.new_context()
            page = await context.new_page()
            await page.goto(url, wait_until="networkidle", timeout=PLAYWRIGHT_TIMEOUT)
            await asyncio.sleep(source.get("render_wait", 1))
            content = await page.content()
            await context.close()
            await browser.close()
            logger.info(f"üé≠ {source['name']}: –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ Playwright")
            return content
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ Playwright –¥–ª—è {source['name']}: {e}")
    return None

def load_processed_urls() -> Set[str]:
    try:
        with open(DUPLICATES_FILE, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    except FileNotFoundError:
        return set()

def save_processed_url(url: str):
    with open(DUPLICATES_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{url}\n")

def get_url_hash(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()

def detect_language(text: str) -> str:
    try:
        if not text or len(text.strip()) < 3:
            return 'unknown'
        return detect(text)
    except LangDetectException:
        return 'unknown'

def translate_to_russian(text: str) -> str:
    if not text or not text.strip():
        return text
    
    try:
        lang = detect_language(text)
        
        if lang == 'ru' or lang == 'unknown':
            return text
        
        translator = GoogleTranslator(source=lang, target='ru')
        
        if len(text) > 4500:
            text = text[:4500]
        
        translated = translator.translate(text)
        return translated if translated else text
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
        return text

async def parse_rss(source: Dict) -> List[Dict]:
    news_items = []
    parsed_count = 0
    filtered_out = 0

    # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
    max_retries = 3
    retry_delay = 2
    
    for retry in range(max_retries):
        try:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ SSL –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            ssl_context.options |= ssl.OP_NO_SSLv2
            ssl_context.options |= ssl.OP_NO_SSLv3
            
            connector = aiohttp.TCPConnector(
                ssl=ssl_context,
                limit=10,
                limit_per_host=5,
                ttl_dns_cache=300,
                use_dns_cache=True,
            )
            
            timeout_settings = aiohttp.ClientTimeout(
                total=60,
                connect=20,
                sock_read=40
            )
            
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout_settings,
                raise_for_status=False
            ) as session:
                async with session.get(
                    source['url'],
                    headers={'User-Agent': 'Mozilla/5.0 (compatible; RSSBot/1.0)'}
                ) as response:
                    if response.status != 200:
                        # HTTP 429 (Too Many Requests) - –Ω—É–∂–Ω–∞ –±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                        if response.status == 429:
                            delay = 30 * (retry + 1)  # 30—Å, 60—Å, 90—Å –¥–ª—è 429
                            if retry < max_retries - 1:
                                logger.warning(f"‚ö†Ô∏è {source['name']}: HTTP 429 (Too Many Requests), –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å (–ø–æ–ø—ã—Ç–∫–∞ {retry+1}/{max_retries})")
                                await asyncio.sleep(delay)
                                continue
                            else:
                                logger.error(f"‚ùå {source['name']}: HTTP 429 –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                                return []
                        # HTTP 403 (Forbidden) –∏–ª–∏ –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
                        elif response.status in [403, 404]:
                            if retry < max_retries - 1:
                                delay = retry_delay * (retry + 1)
                                logger.warning(f"‚ö†Ô∏è {source['name']}: HTTP {response.status}, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å (–ø–æ–ø—ã—Ç–∫–∞ {retry+1}/{max_retries})")
                                await asyncio.sleep(delay)
                                continue
                            else:
                                logger.error(f"‚ùå {source['name']}: HTTP {response.status} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                                return []
                        else:
                            if retry < max_retries - 1:
                                delay = retry_delay * (retry + 1)
                                logger.warning(f"‚ö†Ô∏è {source['name']}: HTTP {response.status}, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å (–ø–æ–ø—ã—Ç–∫–∞ {retry+1}/{max_retries})")
                                await asyncio.sleep(delay)
                                continue
                            else:
                                logger.error(f"‚ùå {source['name']}: HTTP {response.status} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                                return []
                    
                    raw_bytes = await response.read()
                    detected = chardet.detect(raw_bytes)
                    encoding = response.charset or detected.get('encoding') or 'utf-8'
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–æ–∫
                    encoding_mapping = {
                        'windows1251': 'cp1251',
                        'windows-1251': 'cp1251',
                        'cp1251': 'cp1251',
                        'iso-8859-1': 'latin1',
                        'iso8859-1': 'latin1',
                    }
                    encoding = encoding_mapping.get(encoding.lower(), encoding)
                    
                    # –ï—Å–ª–∏ chardet –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª –∏–ª–∏ –¥–∞–ª –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
                    if not encoding or encoding.lower() not in ['utf-8', 'cp1251', 'latin1', 'ascii', 'utf-16']:
                        try:
                            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                            for enc in ['utf-8', 'cp1251', 'latin1']:
                                try:
                                    decoded_content = raw_bytes.decode(enc, errors='strict')
                                    encoding = enc
                                    break
                                except:
                                    continue
                            else:
                                decoded_content = raw_bytes.decode('utf-8', errors='ignore')
                        except:
                            decoded_content = raw_bytes.decode('utf-8', errors='ignore')
                    else:
                        decoded_content = raw_bytes.decode(encoding, errors='ignore')
                    feed_content = sanitize_feed_content(decoded_content)
                    feed = feedparser.parse(feed_content)
                
                if feed.bozo and feed.bozo_exception:
                    logger.warning(f"‚ö†Ô∏è RSS –ø–∞—Ä—Å–∏–Ω–≥ {source['name']}: {feed.bozo_exception}")
                
                if not hasattr(feed, 'entries') or not feed.entries:
                    logger.info(f"üì• {source['name']}: –Ω–∞–π–¥–µ–Ω–æ 0 –∑–∞–ø–∏—Å–µ–π –≤ RSS")
                    return []
                
                total_entries = len(feed.entries[:MAX_NEWS_PER_SOURCE * 2])
                logger.info(f"üì• {source['name']}: –Ω–∞–π–¥–µ–Ω–æ {total_entries} –∑–∞–ø–∏—Å–µ–π –≤ RSS")
                
                for entry in feed.entries[:MAX_NEWS_PER_SOURCE * 2]:
                    parsed_count += 1
                    title = entry.get('title', '').strip()
                    link = entry.get('link', '').strip()
                    description = entry.get('description', '') or entry.get('summary', '') or entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
                    
                    # –û—á–∏—â–∞–µ–º HTML –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                    if description:
                        soup_desc = BeautifulSoup(description, 'html.parser')
                        description = soup_desc.get_text(separator=' ', strip=True)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ —Å—Å—ã–ª–∫–∏
                    if not title or len(title.strip()) < 3 or not link:
                        filtered_out += 1
                        continue
                    
                    combined_text = f"{title} {description}"
                    include_all = source.get('always_include', False)
                    
                    if (include_all or is_relevant(combined_text)) and link:
                        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –æ—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        translated_title = translate_to_russian(title)
                        cleaned_title = clean_title(translated_title)
                        
                        # –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π
                        if not cleaned_title or len(cleaned_title.strip()) < 3:
                            cleaned_title = translated_title.strip() if translated_title else title.strip()
                        
                        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –æ—á–∏—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                        translated_description = translate_to_russian(description)
                        cleaned_description = clean_description(translated_description, cleaned_title)
                        
                        news_items.append({
                            'title': cleaned_title,
                            'description': cleaned_description,
                            'link': link,
                            'source': source['name']
                        })
                        
                        if len(news_items) >= MAX_NEWS_PER_SOURCE:
                            break
                    else:
                        filtered_out += 1
                        
                if parsed_count > 0 and len(news_items) == 0:
                    logger.warning(f"‚ö†Ô∏è {source['name']}: —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {parsed_count}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_out}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö 0")
            
            # –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –≤—ã—Ö–æ–¥–∏–º –∏–∑ retry —Ü–∏–∫–ª–∞
            break
                            
        except (aiohttp.ClientConnectorError, aiohttp.ClientError, asyncio.TimeoutError, ssl.SSLError) as e:
            error_type = type(e).__name__
            if retry < max_retries - 1:
                delay = retry_delay * (retry + 1)
                logger.warning(f"‚ö†Ô∏è {source['name']}: {error_type} (–ø–æ–ø—ã—Ç–∫–∞ {retry+1}/{max_retries}), –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay}—Å: {str(e)[:100]}")
                await asyncio.sleep(delay)
            else:
                logger.error(f"‚ùå {source['name']}: {error_type} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {str(e)[:200]}")
                return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {source['name']}: {type(e).__name__}: {str(e)[:200]}")
            return []
    
    return news_items

async def parse_html(source: Dict) -> List[Dict]:
    news_items = []
    parsed_count = 0
    filtered_out = 0
    
    content = None
    proxy_required = source.get('use_proxy')
    render_js = source.get('render_js')
    last_error = None

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

    for attempt in range(3):
        proxy = get_next_proxy() if proxy_required else None
        try:
            if attempt == 0:
                client_kwargs = dict(verify=False, timeout=30.0, follow_redirects=True)
                if proxy:
                    client_kwargs['proxies'] = proxy
                async with httpx.AsyncClient(**client_kwargs) as client:
                    response = await client.get(source['url'], headers=headers)
                    if response.status_code == 200:
                        content = response.text
                        break
                    else:
                        last_error = f"HTTP {response.status_code}"
                        logger.debug(f"‚ö†Ô∏è {source['name']}: HTTP {response.status_code} –ø—Ä–∏ –ø–µ—Ä–≤–∏—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ")

            elif attempt == 1:
                scraper = cloudscraper.create_scraper(
                    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
                )
                if proxy:
                    scraper.proxies.update({'http': proxy, 'https': proxy})
                response = scraper.get(source['url'], timeout=30)
                if response.status_code == 200:
                    content = response.text
                    break
                else:
                    last_error = f"HTTP {response.status_code}"
                    logger.debug(f"‚ö†Ô∏è {source['name']}: HTTP {response.status_code} —á–µ—Ä–µ–∑ cloudscraper")

            await asyncio.sleep(2)

        except Exception as e:
            last_error = str(e)
            if attempt == 2:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML {source['name']} (–≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏): {e}")
            continue

    if not content and render_js:
        content = await fetch_html_with_playwright(source['url'], source)

    if not content:
        if last_error:
            logger.warning(f"‚ö†Ô∏è {source['name']}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç (–ø–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error})")
        return news_items
    
    try:
        soup = BeautifulSoup(content, 'lxml')
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
        articles = soup.select(source['selector'])[:MAX_NEWS_PER_SOURCE * 2]
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–ø—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        if not articles:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ —Å—Ç–∞—Ç—å–∏/–Ω–æ–≤–æ—Å—Ç–∏
            alternative_selectors = [
                'article', '.article', '.news', '.news-item', 
                '.press-release', '.post', '.entry', '[class*="news"]', '[class*="article"]'
            ]
            for alt_sel in alternative_selectors:
                articles = soup.select(alt_sel)[:MAX_NEWS_PER_SOURCE * 2]
                if articles:
                    logger.info(f"üì• {source['name']}: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä '{alt_sel}'")
                    break
        
        logger.info(f"üì• {source['name']}: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ HTML")
        
        include_all = source.get('always_include', False)

        for article in articles:
            parsed_count += 1
            try:
                title_elem = article.select_one(source['title_selector'])
                link_elem = article.select_one(source['link_selector'])
                desc_elem = article.select_one(source.get('description_selector', ''))
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ fallback –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
                title = ''
                
                # –í–∞—Ä–∏–∞–Ω—Ç 1: –∏–∑ title_selector
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
                    if not title:
                        if title_elem.name == 'a':
                            title = title_elem.get('title', '').strip() or title_elem.get_text(strip=True)
                        else:
                            title = title_elem.get('title', '').strip()
                
                # –í–∞—Ä–∏–∞–Ω—Ç 2: –∏—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–Ω—É—Ç—Ä–∏ article
                if not title or len(title) < 3:
                    alt_title = article.select_one('h1, h2, h3, h4, h5, .title, [class*="title"], [class*="heading"]')
                    if alt_title:
                        title = alt_title.get_text(strip=True)
                
                # –í–∞—Ä–∏–∞–Ω—Ç 3: –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Å—Å—ã–ª–∫–µ, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ link_selector
                if not title or len(title) < 3:
                    if link_elem:
                        title = link_elem.get_text(strip=True) or link_elem.get('title', '').strip()
                        if not title and link_elem.name == 'a':
                            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏
                            title = link_elem.get_text(strip=True)
                
                # –í–∞—Ä–∏–∞–Ω—Ç 4: –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞, –∏—â–µ–º –ª—é–±–æ–π –∑–Ω–∞—á–∏–º—ã–π —Ç–µ–∫—Å—Ç –≤ article
                if not title or len(title) < 3:
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                    for tag in ['strong', 'b', 'p', 'span', 'div']:
                        elem = article.find(tag)
                        if elem:
                            text = elem.get_text(strip=True)
                            if text and len(text) > 10 and len(text) < 300:
                                title = text
                                break
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –µ—Å–ª–∏ link_selector –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if link_elem:
                    link = str(link_elem.get('href', '')) if hasattr(link_elem, 'get') else ''
                elif title_elem and title_elem.name == 'a':
                    link = str(title_elem.get('href', '')) if hasattr(title_elem, 'get') else ''
                else:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ article —ç–ª–µ–º–µ–Ω—Ç–∞
                    link_elem_fallback = article.select_one('a[href]')
                    link = str(link_elem_fallback.get('href', '')) if link_elem_fallback else ''
                
                description = desc_elem.get_text(separator=' ', strip=True) if desc_elem else ''
                
                # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
                if not description or len(description) < 20:
                    full_text = article.get_text(separator=' ', strip=True)
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏, –∏—Å–∫–ª—é—á–∞—è –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    words = full_text.split()
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if title:
                        title_words = title.lower().split()
                        skip_count = 0
                        for i, word in enumerate(words[:min(len(title_words) + 2, len(words))]):
                            if word.lower() in title_words[:3]:
                                skip_count = i + 1
                            else:
                                break
                        if skip_count > 0:
                            words = words[skip_count:]
                    description = ' '.join(words[:50]) if words else ''
                
                # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                if description:
                    description = ' '.join(description.split())
                
                if link and not link.startswith('http'):
                    from urllib.parse import urljoin
                    link = urljoin(source['url'], link)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ —Å—Å—ã–ª–∫–∏ (–ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è)
                if not title or len(title.strip()) < 3 or not link or not link.strip():
                    filtered_out += 1
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
                title = title.strip()
                link = link.strip()
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ)
                combined_text = f"{title} {description}"
                
                if (include_all or is_relevant(combined_text)) and link:
                    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    translated_title = translate_to_russian(title)
                    
                    # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    cleaned_title = clean_title(translated_title)
                    
                    # –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –æ—á–∏—Å—Ç–∫–∞ —É–¥–∞–ª–∏–ª–∞ –≤–µ—Å—å –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π –æ—Ä–∏–≥–∏–Ω–∞–ª
                    if not cleaned_title or len(cleaned_title.strip()) < 3:
                        cleaned_title = translated_title.strip()
                    
                    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –æ—á–∏—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    translated_description = translate_to_russian(description)
                    cleaned_description = clean_description(translated_description, cleaned_title)
                    
                    news_items.append({
                        'title': cleaned_title,
                        'description': cleaned_description,
                        'link': link,
                        'source': source['name']
                    })
                    
                    if len(news_items) >= MAX_NEWS_PER_SOURCE:
                        break
                else:
                    filtered_out += 1
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ –∏–∑ {source['name']}: {e}")
                continue
        
        if parsed_count > 0 and len(news_items) == 0:
            logger.warning(f"‚ö†Ô∏è {source['name']}: —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {parsed_count}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_out}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö 0")
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML {source['name']}: {e}")
    
    return news_items

async def collect_news() -> List[Dict]:
    all_news = []
    
    logger.info(f"üì∞ –ù–∞—á–∞–ª–æ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(NEWS_SOURCES)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    tasks = []
    for source in NEWS_SOURCES:
        if source['type'] == 'rss':
            tasks.append(parse_rss(source))
        elif source['type'] == 'html':
            tasks.append(parse_html(source))
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for idx, result in enumerate(results):
        if isinstance(result, list):
            source_name = NEWS_SOURCES[idx]['name']
            if result:
                logger.info(f"üì∞ {source_name}: —Å–æ–±—Ä–∞–Ω–æ {len(result)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                for item in result[:2]:
                    logger.info(f"   - {item['title'][:60]}...")
            all_news.extend(result)
    
    return all_news

def format_post(news_item: Dict) -> str:
    start_emoji = random.choice(EMOJIS['start'])
    
    title = news_item.get('title', '').strip()
    description = news_item.get('description', '').strip()
    link = news_item.get('link', '')
    
    if not title:
        title = "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ HTML, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è Telegram
    title_escaped = escape(title)
    desc_escaped = escape(description) if description else ''
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ format_post
    if desc_escaped:
        title_normalized = re.sub(r'[^\w\s]', '', title.lower()).strip()
        desc_normalized = re.sub(r'[^\w\s]', '', desc_escaped.lower()).strip()
        
        # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–µ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ–±—Ä–µ–∑–∞–µ–º
        if desc_normalized.startswith(title_normalized):
            desc_words = desc_escaped.split()
            title_words = title.split()
            if len(desc_words) > len(title_words):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞, —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                match_count = min(len(title_words), 5)
                if match_count < len(desc_words):
                    desc_escaped = ' '.join(desc_words[match_count:])
    
    post = f"{start_emoji} <b>{title_escaped}</b>\n\n"
    
    if desc_escaped:
        # –û–±—Ä–µ–∑–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ 500 —Å–∏–º–≤–æ–ª–æ–≤, —Å—Ç–∞—Ä–∞—è—Å—å –Ω–µ —Ä–µ–∑–∞—Ç—å —Å–ª–æ–≤–∞
        if len(desc_escaped) > 500:
            desc_text = desc_escaped[:500].rsplit(' ', 1)[0] + "..."
        else:
            desc_text = desc_escaped
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ
        if desc_text.strip():
            post += f"{desc_text}\n\n"
    
    post += f"<a href='{link}'>–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</a>"
    
    return post

async def publish_news(news_items: List[Dict]):
    processed_urls = load_processed_urls()
    published_count = 0
    duplicates_count = 0
    
    logger.info(f"üìã –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(news_items)}")
    logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ö–µ—à–µ–π –∏–∑ duplicates.txt: {len(processed_urls)}")
    
    for news_item in news_items:
        url_hash = get_url_hash(news_item['link'])
        
        if url_hash in processed_urls:
            duplicates_count += 1
            if duplicates_count <= 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥—É–±–ª–∏–∫–∞—Ç–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                logger.debug(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç: {news_item['title'][:50]}... (URL: {news_item['link'][:50]})")
            continue
        
        try:
            post_text = format_post(news_item)
            
            await bot.send_message(
                chat_id=PREVIEW_CHANNEL_ID,
                text=post_text,
                parse_mode=ParseMode.HTML,
                disable_web_page_preview=False
            )
            
            save_processed_url(url_hash)
            processed_urls.add(url_hash)
            published_count += 1
            
            logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {news_item['title'][:50]}... ({news_item['source']})")
            
            await asyncio.sleep(3)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            await asyncio.sleep(5)
    
    logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {published_count} | üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count} | üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(news_items)}")

async def news_cycle():
    if STATIC_PROXY:
        logger.info(f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–∫—Å–∏: {STATIC_PROXY.split('@')[-1] if '@' in STATIC_PROXY else STATIC_PROXY}")
    else:
        load_proxy_pool()
    logger.info("üîç –ù–∞—á–∞–ª–æ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    news_items = await collect_news()
    logger.info(f"üìä –°–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –í–°–ï–ì–û: {len(news_items)}")
    
    if news_items:
        await publish_news(news_items)
    else:
        logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤!")

    cleanup_logs()

async def main():
    logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∫–∞–∂–¥—ã–µ {CHECK_INTERVAL // 60} –º–∏–Ω—É—Ç")
    
    await news_cycle()
    
    while True:
        await asyncio.sleep(CHECK_INTERVAL)
        await news_cycle()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
